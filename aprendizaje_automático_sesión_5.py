# -*- coding: utf-8 -*-
"""Aprendizaje Automático - Sesión 5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_SbyF4gnpqzPBXQeNAoAIy3a2bQNfDL

<img src='https://drive.google.com/uc?id=1DHE8rHnhKRam8LrZpOkVY6iF3GMK7Jwf' caption="Máster Universitario en Automática y Robótica"></center>

# Aprendizaje Automático
## Sesión 5 - Aprendizaje por refuerzo

Profesor: **Jorge Calvo Zaragoza**

### Resumen
En esta sesión:
  * Veremos el framework *Gym* de OpenAI
  * Resolveremos un problema de *Gym* usando **Deep Q Networks**

# Gym

En esta sesión vamos a describir un pequeño proyecto de programación para resolver un reto robótico utilizando aprendizaje por refuerzo. Se considerará un entorno simulado para representar una versión (tremendamente) simplificada de una tarea real donde se podrían utilizar este tipo de técnicas.

Dado que se trata de una sesión de introducción, vamos a utilizar un entorno simulado de aprendizaje por refuerzo que nos permita realizar pruebas de forma sencilla. En concreto, vamos a utilizar el paquete Gym de OpenAI.

Gym es un paquete de código abierto para desarrollar y comparar algoritmos de aprendizaje por refuerzo. Este paquete proporciona una interfaz común para colecciones de problemas, también llamados *entornos*, que permite escribir algoritmos genéricos. Para una primera introducción a Gym se puede consultar su página de [documentación](https://gym.openai.com/docs/).

## Visualizar Gym en Colab

Gym es un paquete pensado para ser usado en un sistema con escritorio. En particular, la renderización del comportamiento del agente en el entorno (una de sus funcionalidades más utilizadas) requiere una componente de visualización que no está disponible en otros entornos (por ejemplo, en Colab).

El código que aparece a continuación permite volcar el renderizado del entorno en un video mp4, que posteriormente sí puede ser reproducido en Colab.
"""

!pip install gym pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1

import gym
from gym import logger as gymlogger
from gym.wrappers import Monitor
gymlogger.set_level(40) #error only

import math
import glob
import io
import base64
from IPython.display import HTML
from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
display = Display(visible=0, size=(1400, 900))
display.start()

def show_video():
  mp4list = glob.glob('video/*.mp4')
  if len(mp4list) > 0:
    mp4 = mp4list[0]
    video = io.open(mp4, 'r+b').read()
    encoded = base64.b64encode(video)
    ipythondisplay.display(HTML(data='''<video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
  else: 
    print("No se encuentra el video")

def wrap_env(env):
  env = Monitor(env, './video', force=True)
  return env

"""# Entorno CartPole

Vamos a ver cómo resolver el entorno [*CartPole*](https://gym.openai.com/envs/CartPole-v1/) con **DQN**. En este entorno, un péndulo (pole) está sujeto por una articulación inerte a un carro (cart), que se mueve a lo largo de un eje horizontal. 


El sistema se controla aplicando una fuerza de 1 o -1 (acciones discretas) al carro. El péndulo comienza en posición vertical y el objetivo es evitar que se caiga. 

<figure>
<center>
<img width="500px" src='https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/poster.jpg' />
<figcaption>CartPole</figcaption></center>
</figure>

Se proporciona una recompensa de 1 por cada paso de tiempo que el péndulo permanece en posición vertical. El episodio termina cuando el péndulo está a más de 15 grados del eje vertical o el carro se desplaza fuera de la ventana.

El estado que nos proporcina el entorno contiene 4 valores continuos:
  * Posición del carro
  * Velocidad del carro
  * Ángulo del péndulo
  * Velocidad angular del péndulo

### Ejemplo episodio aleatorio

Para tener mejor idea de qué ocurre en el entorno y entender la sintáxis de Gym, vamos a hacer una ejecución con acciones aleatorias.
"""

import gym

#env = gym.make('CartPole-v1') # Para el uso habitual de Gym en consola
env = wrap_env(gym.make('CartPole-v1'))

state = env.reset() # Inicializacion
acc_reward = 0      # Recompensas acumuladas

# Bucle por cada instante
while True:
    env.render(mode='rgb_array')

    # Accion aleatoria  
    action = env.action_space.sample() 

    # Ejecución
    next_state, reward, done, _ = env.step(action)

    # Acumular recompoensa 
    acc_reward += reward

    # Comprobamos si ha terminado el episodio
    if done:
        print("Retorno: {}".format(acc_reward))
        break
    else:
        state = next_state

env.close()

# Visualiza el ultimo episodio (desde el env.reset() hasta env.close())
show_video()

"""## DQN para CartPole

**Para las redes neuronales de esta sesión vamos a utilizar una versión antigua de Tensorflow y Keras (antes de ser tf.keras)**


Esto es debido a que la implementación de DQN usada en este ejemplo (simplificada para un mejor entendimiento) resulta muy ineficiente para los ajustes de TF 2.x.
"""

!pip install tensorflow==1.14
!pip install Keras==2.3.1

import keras

"""### Red neuronal

Definimos la red neuronal. Las características principales que debe tener son:
* La entrada tiene tantas neuronas como características tiene el espacio de estados.
* La salida tiene tantas neuronas como posibles acciones del entorno.

Los hiper-parámetros de la red neuronal (número de capas, filtros, activaciones...) son arbitrarios, aunque pueden influir en la convergencia y éxito del aprendizaje.
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam

def get_model(env):
    dim_states = env.observation_space.shape[0]   # == 4
    dim_actions = env.action_space.n              # == 2

    model = Sequential()
    model.add(Dense(32, activation='relu', input_dim=dim_states))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(dim_actions, activation='linear'))                     
    model.compile(loss='mse', optimizer=Adam())
    return model

"""### Política 

Vamos a definir una política $\epsilon$-voraz. Sacaremos un número al azar y si no supera el valor de $\epsilon$, tomaremos una acción al azar. En caso contrario, se escogerá la acción que mayor activación tiene según la DQN.
"""

def policy(model, state, epsilon):

    # Accion aleatoria
    if np.random.rand() <= epsilon:
        return env.action_space.sample()        # Accion al azar

    # Accion voraz
    action_activations = model.predict(state)
    return np.argmax(action_activations[0])     # Accion voraz

"""### Experience Replay

Como hemos visto en teoría, DQN se utiliza junto al mecanismo de *Experience Replay* donde la estimación de los valores para los pares estado-acción no se realiza sobre las experiencias recientes sino muestreando una memoria con un número (finito) de experiencias pasadas.
"""

def experience_replay(model_learning, model_target, memory, batch_size, gamma):
  # Extraemos un conjunto de experiencias aleatorias de la memoria
  batch = [random.choice(memory) for _ in range(batch_size)]

  # Cada memoria se compone de un (s,a,r,s') y si se acababa el episodio después
  for m_state, m_action, m_reward, m_next_state, m_done in batch:

      if m_done:
          # Si hemos acabado no hay retorno posterior
          target = m_reward
      else:
          # Regla de actualización de Q-Learning
          target = m_reward + gamma * np.amax(model_target.predict(m_next_state)[0])

      # Valores actuales del modelo
      target_f = model_learning.predict(m_state) 

      # Actualizamos el de la accion que se escogió
      target_f[0][m_action] = target  

      # Hacemos una pasada de aprendizaje
      model_learning.fit(m_state, target_f, epochs=1, verbose=0)

"""### DQN

En el siguiente código definimos las variables de configuración..
"""

# ======================================================
# Selección de parámetros
#

GAMMA = 0.95          # Factor de descuento
EPISODES = 50         # Numero maximo de episodios
HISTORY_SIZE = 2000   # Tamaño de la memoria para Experience Replay
BATCH_SIZE = 16       # Tamaño del batch para Experience Replay
EPSILON_INIT = 1.0    # epsilon inicial
EPSILON_DECAY = 0.95  # delta

"""Ahora realizamos el proceso de interacción con el entorno, integrando el algoritmo DQN."""

import random
import numpy as np
from collections import deque

# ======================================================
# Proceso
#

env = gym.make('CartPole-v1')

# Inicialización
model_learning = get_model(env)       # Red que se aprende
model_target = get_model(env)         # Red objetivo

memory = deque(maxlen=HISTORY_SIZE)   # Memoria para experience replay
epsilon = EPSILON_INIT                # Epsilon para cada episodio

# Monitorizar retorno de ultimos 10 episodios
last_rewards = deque(maxlen=10)

# Bucle por episodio
for e in range(EPISODES):
    state = env.reset()
    state = np.reshape(state, [1, len(state)]) # ==> [1,4]
    acc_reward = 0

    # Entorno no episodico
    while True:
        # Elegimos accion y ejecutamos
        action = policy(model_learning, state, epsilon)
        next_state, reward, done, _ = env.step(action)
        next_state = np.reshape(next_state, [1, len(next_state)]) # ==> [1,4]

        # Almacenamos la experiencia y acumulamos la recompensa
        memory.append([state, action, reward, next_state, done])
        acc_reward += reward

        #   Aprendizaje !
        experience_replay(model_learning, model_target, memory, BATCH_SIZE, GAMMA)

        if done:
            print("episodio: {}, retorno: {}, retorno prom. 10: {:.1f}, replay: {}, epsilon: {:.3f}".format(e, acc_reward,np.mean(last_rewards),len(memory),epsilon))

            # Guardamos recompensa en el historico
            last_rewards.append(acc_reward)

            # Actualizamos la red objetivo !
            model_target.set_weights(model_learning.get_weights())

            
            break
        else:
            state = next_state
       
    epsilon *= EPSILON_DECAY
    env.close()

"""### Comprobación

Por último, vamos a hacer una última trayectoria el agente para poder renderizarla y ver qué está ocurriendo.
"""

env = wrap_env(gym.make('CartPole-v1'))
state = env.reset()
state = np.reshape(state, [1, len(state)])
acc_reward = 0

# Bucle por paso
while True:

    env.render()

    # Elegimos accion y ejecutamos
    action = policy(model_learning, state, epsilon)
    next_state, reward, done, _ = env.step(action)
    next_state = np.reshape(next_state, [1, len(next_state)])

    # Almacenamos la experiencia y acumulamos la recompensa
    acc_reward += reward

    # -----------------------------

    if done:
        print("Final episode: {} reward".format(acc_reward))   
        break
    else:
        state = next_state

env.close()
show_video()