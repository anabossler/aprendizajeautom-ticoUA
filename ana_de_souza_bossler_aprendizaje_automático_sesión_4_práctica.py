# -*- coding: utf-8 -*-
"""Ana de Souza Bossler Aprendizaje Automático - Sesión 4 - Práctica

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Op0W5UGtgu68G0kEe0tqibo3IOmTLtGq

<img src='https://drive.google.com/uc?id=1DHE8rHnhKRam8LrZpOkVY6iF3GMK7Jwf' caption="Máster Universitario en Automática y Robótica"></center>

# Aprendizaje Automático
## Práctica 4

Profesor: **Jose Javier Valero Mas**

### Ejercicio

La tarea a realizar consistirá en poner en práctica los conceptos prácticos introducidos en esta sesión.

Para ello el alumno deberá:

* Cargar la base de datos ***MNIST*** utilizando la propia función de carga de `keras`. 
* En lugar de trabajar con los datos iniciales se harán dos métodos de *Representation learning*, **PCA** y **Autoencoder** con las siguientes premisas:
  * Ambos algoritmos deberán devolver la misma cantidad de características.
  * El **Autoencoder** deberá contener una única capa oculta.

* En base a ello se propone que el alumno entrene y pruebe diferentes algoritmos de clasificación en el nuevo espacio de representación, reportando el que está obteniendo mejores resultados.

### Entrega

  * Haz una copia de este archivo ("Archivo" > "Guardar una copia en drive").

  * Realiza la práctica descrita en el apartado anterior sobre el nuevo archivo. Documenta los pasos que vas realizando incrustando texto en Colab.

  * Genera un enlace para compartir (acuérdate de dar los permisos apropiados). Copia y pega el enlace en la entrega habilitada en el Moodle de la asignatura.

1. Cargar la base de datos MNIST utilizando la propia función de carga de keras.
"""

import tensorflow as tf

# Cargamos la base de datos desde el API de Keras
(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()

# Dimensiones de los datos
print("Datos de entrenamiento - Imagenes: {} - Etiquetas: {}".format(X_train.shape,Y_train.shape))
print("Datos de test - Imagenes: {} - Etiquetas: {}".format(X_test.shape,Y_test.shape))
print()


# Redimensionamos las imágenes para que sean como un vector de características
# De (N, 28, 28) a (N, 784)
X_train = X_train.reshape(-1, 784)
X_test = X_test.reshape(-1, 784)

"""2.Primero procedemos con PCA, eligiendo 8 características. Separamos los datos de test (Xis) y de entreinamiento (X). Estratificamos Y_train, y, embarajamos los datos para que sean aleatorios. """

from sklearn.decomposition import PCA

model = PCA(n_components=8)
model.fit(X_train)
X = model.transform(X_train)

model = PCA(n_components=8)
model.fit(X_test)
Xis = model.transform(X_test)

import sklearn
sklearn.model_selection.train_test_split(X, Y_train, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=Y_train)

"""3. Procedemos con la aplicación del modelo de Decision Tree."""

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score

classifier = DecisionTreeClassifier()

cls_PCA = classifier.fit(X, Y_train)

prediction_PCA = cls_PCA.predict(Xis)

cls_raw = classifier.fit(X, Y_train)
prediction_raw = cls_raw.predict(Xis)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_PCA)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score

classifier = RandomForestClassifier()

cls_PCA = classifier.fit(X, Y_train)

prediction_PCA = cls_PCA.predict(Xis)

cls_raw = classifier.fit(X, Y_train)
prediction_raw = cls_raw.predict(Xis)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_PCA)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score

classifier = KNeighborsClassifier()

cls_PCA = classifier.fit(X, Y_train)

prediction_PCA = cls_PCA.predict(Xis)

cls_raw = classifier.fit(X, Y_train)
prediction_raw = cls_raw.predict(Xis)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_PCA)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

"""Vemos que la tasa de aprendizaje es muy baja, aunque sea más alta con KNN que con el decision tree , por lo que utilizaremos 32 características.

"""

model = PCA(n_components=32)
model.fit(X_train)
X = model.transform(X_train)

model = PCA(n_components=32)
model.fit(X_test)
Xis = model.transform(X_test)

import sklearn
sklearn.model_selection.train_test_split(X, Y_train, test_size=0.2, train_size=0.8, random_state=42, shuffle=True, stratify=Y_train)

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score

classifier = DecisionTreeClassifier()

cls_PCA = classifier.fit(X, Y_train)

prediction_PCA = cls_PCA.predict(Xis)

cls_raw = classifier.fit(X, Y_train)
prediction_raw = cls_raw.predict(Xis)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_PCA)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score

classifier = RandomForestClassifier()

cls_PCA = classifier.fit(X, Y_train)

prediction_PCA = cls_PCA.predict(Xis)

cls_raw = classifier.fit(X, Y_train)
prediction_raw = cls_raw.predict(Xis)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_PCA)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

"""La tasa de acierto es todavía más baja con 32 elementos,  por lo que procedemos a la aplicación del autoencoder. Primero con 8 y luego con 32 dimensiones.

Con autoencoders y 1 capa oculta, 3 dimensiones
"""

#Primero de todo normalizamos los datos (rango [0,1]):
X_train_norm = X_train.astype('float32')/255

#Modelo de red:
###Input layer:
input_img = tf.keras.layers.Input(shape=(784,))
###First hidden layer - Compacting data:
first_encoding_layer = tf.keras.layers.Dense(16,'relu')(input_img)
###Second hidden layer - Encoded representation:
encoded = tf.keras.layers.Dense(8,'linear')(first_encoding_layer)
###Third hidden layer - Expanding data:
first_decoding_layer = tf.keras.layers.Dense(16,'relu')(encoded)
###Reconstruction layer:
decoded = tf.keras.layers.Dense(784,'sigmoid')(first_decoding_layer)

#Creamos el autoencoder (red completa):
autoencoder_model = tf.keras.Model(inputs=[input_img], outputs=[decoded], name='Autoencoder')

#Creamos la parte del encoder:
encoder_model = tf.keras.Model(inputs=[input_img], outputs=[encoded], name='Encoder')

#Mostramos las informaciones de cada uno de los modelos:
autoencoder_model.summary()
print()
encoder_model.summary()

autoencoder_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=['MSE']
)

history = autoencoder_model.fit(x=X_train_norm, y=X_train_norm, validation_split=0.25, epochs=15, verbose=2)

#Aplicamos el Encoder:
X_train_encoded = encoder_model.predict(X_train_norm)

#Mostramos los datos:
print("Pre-Encoder:\t{} filas con {} características".format(X_train_norm.shape[0], X_train_norm.shape[1]))
print("Post-Encoder:\t{} filas con {} características".format(X_train_encoded.shape[0], X_train_encoded.shape[1]))

#Normalizamos los datos de entrada (train y test):
X_train_norm = X_train.astype('float32')/255
X_test_norm = X_test.astype('float32')/255

#Convertimos los datos de su espacio original al espacio del Encoder:
X_train_encoded = encoder_model.predict(X_train_norm)
X_test_encoded = encoder_model.predict(X_test_norm)

#Mostramos los datos:
print("Train:\t{} filas con {} características".format(X_train_encoded.shape[0], X_train_encoded.shape[1]))
print("Test:\t{} filas con {} características".format(X_test_encoded.shape[0], X_test_encoded.shape[1]))

"""Con el modelo de árbol de decisión"""

#Importamos el modelo de árbol de decisión:
from sklearn.tree import DecisionTreeClassifier

#Importamos la métrica para medir la bondad del clasificador:
from sklearn.metrics import accuracy_score

classifier = DecisionTreeClassifier()

#Entrenamos el clasificador:
cls_encoded = classifier.fit(X_train_encoded, Y_train)

#Realizamos la predicción:
prediction_encoded = cls_encoded.predict(X_test_encoded)


#Entrenamos el clasificador:
cls_raw = classifier.fit(X_train, Y_train)

#Realizamos la predicción:
prediction_raw = cls_raw.predict(X_test)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_encoded)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

#Importamos el modelo de árbol de decisión:

from sklearn.ensemble import RandomForestClassifier

#Importamos la métrica para medir la bondad del clasificador:
from sklearn.metrics import accuracy_score

classifier = RandomForestClassifier()


#Entrenamos el clasificador:
cls_encoded = classifier.fit(X_train_encoded, Y_train)

#Realizamos la predicción:
prediction_encoded = cls_encoded.predict(X_test_encoded)


#Entrenamos el clasificador:
cls_raw = classifier.fit(X_train, Y_train)

#Realizamos la predicción:
prediction_raw = cls_raw.predict(X_test)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_encoded)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

from sklearn.neighbors import KNeighborsClassifier


#Importamos la métrica para medir la bondad del clasificador:
from sklearn.metrics import accuracy_score

classifier = KNeighborsClassifier()


#Entrenamos el clasificador:
cls_encoded = classifier.fit(X_train_encoded, Y_train)

#Realizamos la predicción:
prediction_encoded = cls_encoded.predict(X_test_encoded)


#Entrenamos el clasificador:
cls_raw = classifier.fit(X_train, Y_train)

#Realizamos la predicción:
prediction_raw = cls_raw.predict(X_test)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_encoded)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

"""Procedemos con 32 características"""

#Primero de todo normalizamos los datos (rango [0,1]):
X_train_norm = X_train.astype('float32')/255

#Modelo de red:
###Input layer:
input_img = tf.keras.layers.Input(shape=(784,))
###First hidden layer - Compacting data:
first_encoding_layer = tf.keras.layers.Dense(64,'relu')(input_img)
###Second hidden layer - Encoded representation:
encoded = tf.keras.layers.Dense(32,'linear')(first_encoding_layer)
###Third hidden layer - Expanding data:
first_decoding_layer = tf.keras.layers.Dense(64,'relu')(encoded)
###Reconstruction layer:
decoded = tf.keras.layers.Dense(784,'sigmoid')(first_decoding_layer)

#Creamos el autoencoder (red completa):
autoencoder_model = tf.keras.Model(inputs=[input_img], outputs=[decoded], name='Autoencoder')

#Creamos la parte del encoder:
encoder_model = tf.keras.Model(inputs=[input_img], outputs=[encoded], name='Encoder')

#Mostramos las informaciones de cada uno de los modelos:
autoencoder_model.summary()
print()
encoder_model.summary()

autoencoder_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=['MSE']
)

history = autoencoder_model.fit(x=X_train_norm, y=X_train_norm, validation_split=0.25, epochs=15, verbose=2)

#Aplicamos el Encoder:
X_train_encoded = encoder_model.predict(X_train_norm)

#Mostramos los datos:
print("Pre-Encoder:\t{} filas con {} características".format(X_train_norm.shape[0], X_train_norm.shape[1]))
print("Post-Encoder:\t{} filas con {} características".format(X_train_encoded.shape[0], X_train_encoded.shape[1]))

#Normalizamos los datos de entrada (train y test):
X_train_norm = X_train.astype('float32')/255
X_test_norm = X_test.astype('float32')/255

#Convertimos los datos de su espacio original al espacio del Encoder:
X_train_encoded = encoder_model.predict(X_train_norm)
X_test_encoded = encoder_model.predict(X_test_norm)

#Mostramos los datos:
print("Train:\t{} filas con {} características".format(X_train_encoded.shape[0], X_train_encoded.shape[1]))
print("Test:\t{} filas con {} características".format(X_test_encoded.shape[0], X_test_encoded.shape[1]))

#Importamos el modelo de árbol de decisión:
from sklearn.tree import DecisionTreeClassifier

#Importamos la métrica para medir la bondad del clasificador:
from sklearn.metrics import accuracy_score

classifier = DecisionTreeClassifier()

#Entrenamos el clasificador:
cls_encoded = classifier.fit(X_train_encoded, Y_train)

#Realizamos la predicción:
prediction_encoded = cls_encoded.predict(X_test_encoded)


#Entrenamos el clasificador:
cls_raw = classifier.fit(X_train, Y_train)

#Realizamos la predicción:
prediction_raw = cls_raw.predict(X_test)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_encoded)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

#Importamos el modelo de árbol de decisión:

from sklearn.ensemble import RandomForestClassifier

#Importamos la métrica para medir la bondad del clasificador:
from sklearn.metrics import accuracy_score

classifier = RandomForestClassifier()


#Entrenamos el clasificador:
cls_encoded = classifier.fit(X_train_encoded, Y_train)

#Realizamos la predicción:
prediction_encoded = cls_encoded.predict(X_test_encoded)


#Entrenamos el clasificador:
cls_raw = classifier.fit(X_train, Y_train)

#Realizamos la predicción:
prediction_raw = cls_raw.predict(X_test)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_encoded)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

from sklearn.neighbors import KNeighborsClassifier


#Importamos la métrica para medir la bondad del clasificador:
from sklearn.metrics import accuracy_score

classifier = KNeighborsClassifier()


#Entrenamos el clasificador:
cls_encoded = classifier.fit(X_train_encoded, Y_train)

#Realizamos la predicción:
prediction_encoded = cls_encoded.predict(X_test_encoded)


#Entrenamos el clasificador:
cls_raw = classifier.fit(X_train, Y_train)

#Realizamos la predicción:
prediction_raw = cls_raw.predict(X_test)

print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_encoded)))
print("La tasa de acierto obtenida es {:.2f}%".format(100*accuracy_score(Y_test, prediction_raw)))

"""Del análisis hecho podemos ver que el autoencoder tiene un desempeño superior a PCA y puede significar que los datos no sean lineales, aunque tengan el mismo número de características y solo tengamos una capa de activación (que junto con la función) debería dar resultados muy parecidos a la PCA. Tendríamos que analizar los tipos de vectores (ya que son de naturaleza diferente) pero en teoría deberían dar resultados semejantes por la única capa de activación. """